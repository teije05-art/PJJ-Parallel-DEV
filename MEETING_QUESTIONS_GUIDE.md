# Strategic Questions Guide for Partner Meeting
**Nov 20, 2025 | KPMG Tax Team Workflow Discussion**

---

## HOW TO USE THIS GUIDE

This is a **conversation guide**, not a script.

**Your Approach:**
1. Start with context: "I want to understand your workflow so I can tailor Jupiter perfectly"
2. Ask questions naturally (don't sound like you're reading from a list)
3. Take notes on their answers
4. Listen for pain points - those are your gold
5. After listening, present your proposal

**Key Listening Tips:**
- When they mention a pain point (time, quality, consistency), drill deeper
- Notice which steps take the most time - those are highest ROI for automation
- Ask "why" and "how many" to get quantifiable data

---

## OPENING STATEMENT (Start with this)

*"I've been thinking about how we can tailor Jupiter specifically for your tax team. From what you shared, your workflow has three clear steps: classifying questions, searching your database, and building responses. Before I propose something, I want to really understand your process in detail so we can optimize where it matters most. Mind if I ask some questions?"*

---

## SECTION 1: TOPIC CLASSIFICATION (Step 1)

**Goal**: Understand how questions become topics, and whether this is a candidate for automation

### Opening Questions
- [ ] "How do you classify a client question? Is there a standard list of topics you use?"
- [ ] "Is this step consistent across your whole team, or does it vary?"

### If they say it varies:
- [ ] "When people classify the same question differently, what causes that?"
- [ ] "Would standardizing this help your team?"

### Follow-up questions
- [ ] "On average, how long does classification take per question?"
- [ ] "Can a single question map to multiple topics, or just one?"
- [ ] "Is there a topic that comes up most frequently?"

### What you're listening for:
- âœ“ If it's manual, that's where Jupiter could automate/standardize
- âœ“ If there are 10+ standard topics, Jupiter can have a dropdown
- âœ“ Time spent here = potential savings, but usually small
- âœ“ Inconsistency = quality problem Jupiter can solve

---

## SECTION 2: DOCUMENT SEARCH (Step 2) â­ THE CRITICAL ONE

**Goal**: This is where 90% of the time is spent. Understand pain points deeply.

### Opening Questions
- [ ] "Walk me through a recent client question. How many documents did you end up reviewing before finding what you needed?"
- [ ] "How long does the search process typically take for a question like that?"
- [ ] "What makes a document 'relevant' vs. 'not relevant' in your field?"

### If they say it takes a long time:
- [ ] "What takes the longest part of the search? Finding docs, or deciding which ones are relevant?"
- [ ] "Do you ever wish you could search more efficiently?"

### About their database
- [ ] "How is your database organized currently? By topic? Date? Type?"
- [ ] "Do your documents have metadata? Tags? Categories?"
- [ ] "Approximately how many documents are we talking about?"
- [ ] "How often is the database updated? New docs added regularly?"

### About search strategy
- [ ] "When you search, do you start with keywords, or do you have a browse strategy?"
- [ ] "Are there certain documents you search for repeatedly?"
- [ ] "Do you ever miss important documents in your search?"
- [ ] "Are some searches harder than others?"

### About team consistency
- [ ] "If two different team members search for the same topic, do they find the same documents?"
- [ ] "Do they find documents in the same order/priority?"
- [ ] "Is that okay, or would standardization help?"

### What you're listening for:
- âœ“ Time spent: If >30 min per search, Jupiter saves huge time
- âœ“ Frustration points: What annoys them?
- âœ“ Repeated searches: If certain topics come up often, those are good patterns to learn
- âœ“ Missed documents: If they're concerned about missing things, semantic search is valuable
- âœ“ Database size: Bigger database = more benefit from automated search
- âœ“ Metadata: If documents are tagged, Jupiter can be even smarter

---

## SECTION 3: RESPONSE BUILDING (Step 3)

**Goal**: Understand the output quality requirements and approval process

### Opening Questions
- [ ] "Once you have your documents, how do you build the client response?"
- [ ] "Do you copy sections directly from the documents, or do you synthesize?"
- [ ] "Who writes the final response? The person who searched, or someone else?"

### About response format
- [ ] "What format do clients expect? A memo? An analysis? A presentation?"
- [ ] "Is there a standard structure you follow, or does it vary by question?"
- [ ] "How long are typical responses? A few pages? 10+ pages?"

### About approval/quality control
- [ ] "Who reviews the response before it goes to the client?"
- [ ] "What are they looking for? Accuracy? Completeness? Both?"
- [ ] "How often do clients come back asking for clarifications or corrections?"
- [ ] "Are there common errors you catch in the review process?"

### Timing
- [ ] "How long does Step 3 (building the response) typically take?"
- [ ] "Is that time spent researching/reading, or writing/formatting?"

### What you're listening for:
- âœ“ If responses are synthesized (not just copy-paste), Jupiter can help
- âœ“ If there's an approval step, that's where human-in-the-loop gates go
- âœ“ If there's a standard format, Jupiter can generate that
- âœ“ If corrections happen, Jupiter's synthesis approach may prevent that
- âœ“ Time spent here = additional savings

---

## SECTION 4: PAIN POINTS & BOTTLENECKS

**Goal**: Identify the highest-impact improvement areas

### The Direct Question
- [ ] "If you could improve one part of this process, what would it be?"

### If they mention time:
- [ ] "Which step takes the most time? The search or the compilation?"
- [ ] "If we could cut that in half, how much would that help your team?"

### If they mention consistency:
- [ ] "How do you currently ensure all responses meet the same quality standard?"
- [ ] "Is inconsistency a problem you deal with?"
- [ ] "What causes the inconsistency?"

### If they mention quality:
- [ ] "What's the risk if a response is incomplete or misses something?"
- [ ] "How much time do you spend double-checking to make sure nothing was missed?"

### If they mention team scale:
- [ ] "How many people are doing this work today?"
- [ ] "How much of their day is spent on search-and-compile vs. other work?"
- [ ] "Is this a bottleneck for your team's capacity?"

### If they mention knowledge transfer:
- [ ] "When you hire new people, how long does it take them to get good at searching?"
- [ ] "Do junior people find different documents than senior people?"

### What you're listening for:
- âœ“ Time = you can quantify ROI in hours saved
- âœ“ Consistency = Jupiter provides repeatability
- âœ“ Quality = Jupiter's synthesis approach can improve accuracy
- âœ“ Scalability = If capacity is bottleneck, Jupiter multiplies team effectiveness
- âœ“ Knowledge transfer = System learning captures institutional knowledge

---

## SECTION 5: DATABASE & IMPLEMENTATION SETUP

**Goal**: Understand technical requirements and constraints

### About the data you received
- [ ] "The files you gave us - PDFs and CSVs - that's the complete database?"
- [ ] "Is it organized in a way that makes sense, or is it pretty raw?"
- [ ] "Do these files get updated? How frequently? Who manages them?"

### About integration
- [ ] "Do responses need to be integrated into any existing systems?"
- [ ] "Do they go back into SharePoint? A case management system? Email?"
- [ ] "Who needs to see what? Everyone? Just tax team? Specific people?"

### About security & compliance
- [ ] "Are there any security or compliance requirements we need to know about?"
- [ ] "Can data stay on your local system (like a laptop)?"
- [ ] "Or does it need to be on a server?"
- [ ] "Are there any audit/compliance requirements for tracking what we analyzed?"

### What you're listening for:
- âœ“ Data completeness: Do they have everything you need?
- âœ“ Data quality: Is it organized, or is work needed?
- âœ“ Integration points: Where does output go?
- âœ“ Security constraints: Local vs. server, data governance
- âœ“ Audit requirements: Jupiter provides complete audit trails (good!)

---

## SECTION 6: TEAM & ADOPTION

**Goal**: Understand who you're building for and what adoption looks like

### Team size and roles
- [ ] "How many people on the tax team would use this?"
- [ ] "Are they all the same level (all senior, mix of junior/senior, etc.)?"
- [ ] "Who would be your main point of contact for feedback?"

### Current tools & tech comfort
- [ ] "What tools do they use daily? SharePoint, Word, Excel, something else?"
- [ ] "Are they generally tech-savvy, or prefer simpler tools?"
- [ ] "What training or support would they need?"

### Adoption timeline
- [ ] "What's your timeline? Next week? Next month? Next quarter?"
- [ ] "Is there a busy season where this would be especially valuable?"
- [ ] "When would be the best time to roll this out to the team?"

### What you're listening for:
- âœ“ Team size: Affects scope and training needs
- âœ“ Tech comfort: Streamlit interface is simple, but you may need training
- âœ“ Timeline: Sets your implementation schedule
- âœ“ Busy seasons: Opportunity to show value quickly if timing is right

---

## SECTION 7: SUCCESS CRITERIA

**Goal**: Define what "success" looks like so you can measure it

### The Outcome Question
- [ ] "What would success look like to you?"

### Specific metrics to explore
- [ ] "Would it be faster response times?"
- [ ] "Better quality responses?"
- [ ] "Happier clients (fewer revision requests)?"
- [ ] "Your team having more capacity?"
- [ ] "Some combination of these?"

### If they mention time:
- [ ] "If we got response time from 2 hours to 15 minutes, would that be valuable?"
- [ ] "What would that free up your team to do?"

### If they mention quality:
- [ ] "How would we measure quality improvement?"
- [ ] "Are there metrics you use currently?"

### If they mention capacity:
- [ ] "How many more client questions could your team handle if the process was 10x faster?"
- [ ] "What's the revenue impact of that?"

### What you're listening for:
- âœ“ Clear success metrics = you have measurable goals
- âœ“ Quantifiable benefits = strong business case
- âœ“ Multiple benefits = strong ROI story
- âœ“ What they prioritize = where to optimize

---

## TRANSITION TO PROPOSAL

**After you've asked your questions and taken notes:**

*"Okay, I've got a really good picture now. Based on what you've described, here's how I see Jupiter helping..."*

Then go into your proposal:

1. **Map their process to Jupiter's capabilities**
   - Topic classification â†’ Streamlit interface with guided selection
   - Document search â†’ Automated semantic search across entire database
   - Response building â†’ AI synthesis of relevant docs

2. **Show the time savings**
   - Current: [Time they told you]
   - With Jupiter: 5-15 minutes total
   - Savings: [Math]

3. **Address their specific pain points**
   - If they mentioned consistency: "Jupiter provides the same approach every time"
   - If they mentioned missed docs: "Semantic search finds docs humans might miss"
   - If they mentioned team capacity: "Same team can handle 3-5x more questions"

4. **Propose implementation**
   - Phase 1 (Database import): 3-5 days
   - Phase 2 (Testing): 5-7 days
   - Phase 3 (Launch): 3-5 days
   - Total: 2-3 weeks to full deployment

5. **Offer next steps**
   - "I can start importing your database this week"
   - "We'll test with 10-15 real questions"
   - "You'll see results in 1-2 weeks"
   - "Then we can decide on full rollout"

---

## CLOSING QUESTIONS

If the partner seems interested:

- [ ] "Does this approach seem aligned with what you need?"
- [ ] "What concerns do you have?"
- [ ] "What would need to happen for you to move forward?"
- [ ] "Can we start with the database import next week?"

---

## NOTES SECTION

**Use this space to take notes during the meeting:**

### Key Facts
- Team size: ___
- Time per question (current): ___
- Database size (# documents): ___
- Main pain point: ___

### Specific Pain Points Mentioned
1. _____
2. _____
3. _____

### Success Metrics They Care About
1. _____
2. _____
3. _____

### Technical Constraints
- _____
- _____

### Timeline
- Preferred start date: ___
- Preferred launch date: ___

### Next Steps Agreed
- [ ] _____
- [ ] _____

---

**Remember**: This is a conversation, not an interrogation.
Let it flow naturally. If you get the sense they're done talking about a topic, move on.
The goal is to genuinely understand their workflow so you can build something valuable for them.

**Good luck! ðŸŽ¯**
