# Project Jupiter Learning Orchestrator

PDDL-INSTRUCT-inspired planning system adapted for inference-only learning.

## ğŸ¯ **Two Ways to Use**

### **1. Claude Desktop (Recommended - User Friendly!)**
Talk naturally to Claude to approve/reject plans:
```
"Start a planning iteration for deploying orchestrator"
"Approve this plan"
"Reject because it skips testing"
```
**See:** `CLAUDE_USAGE.md`

### **2. Terminal (Developer Mode)**
Direct command-line interface:
```bash
python orchestrator.py
```
**See:** `QUICKSTART.md`

Both use the same learning system!

---

## Overview

This orchestrator implements the MIT paper's approach to teaching LLMs to plan, but adapted for inference-only (no fine-tuning needed). Learning happens through:

1. **Chain-of-thought reasoning**: Explicit state-action-state transitions
2. **Validation feedback**: MemAgent validates each step like VAL in the paper
3. **Human approval**: User feedback provides training signal (via Claude or terminal)
4. **Memory accumulation**: Each iteration adds learned context

## How It Works

```
ITERATION 1: Minimal context
  â†“
Generate plan with CoT â†’ Validate with MemAgent â†’ Get human approval
  â†“
Write to memory (execution_log.md, successful_patterns.md)
  â†“
ITERATION 2: Now has learned context!
  â†“
Generate BETTER plan (learns from iteration 1) â†’ Validate â†’ Approve
  â†“
More memory accumulation
  â†“
ITERATION N: System is now expert at planning!
```

## Resource Requirements

### Mac (Fireworks):
- **VRAM**: 0 GB (uses API)
- **RAM**: ~500 MB (orchestrator script)
- **Cost**: Fireworks API calls

### H100 Instance (vLLM):
- **VRAM**: 1-2 GB temporary per inference call
- **Total**: 80 GB (existing) + 1-2 GB (orchestrator) = 81-82 GB / 90 GB âœ…
- **RAM**: ~500 MB (orchestrator script)

## Quick Start

### 1. Run on Mac (Testing)

```bash
cd /Users/teije/Desktop/memagent/mem-agent-mcp/orchestrator
python orchestrator.py
```

### 2. Run on H100 Instance (Production)

```bash
# Copy to instance
scp -r orchestrator/ user@h100-instance:/path/to/mem-agent-mcp/

# Run on instance
cd /path/to/mem-agent-mcp/orchestrator
python orchestrator.py
```

The script auto-detects backend:
- **macOS**: Uses Fireworks
- **Linux**: Uses vLLM

## Example Session

```
ğŸš€ Project Jupiter Learning Orchestrator
================================================================================
This system uses PDDL-INSTRUCT-inspired learning
Gets smarter with each iteration through memory accumulation
================================================================================

ğŸ¯ Enter planning goal: Implement orchestrator infrastructure

================================================================================
ğŸ”„ ITERATION 1/15
================================================================================

ğŸ“š STEP 1: Retrieving context from memory...
   âœ“ Current status retrieved
   âœ“ Successful patterns: 42 chars (first iteration)
   âœ“ Errors to avoid: 38 chars (no failures yet)

ğŸ§  STEP 2: Generating plan with chain-of-thought reasoning...
   âœ“ Plan generated (1250 chars)
   âœ“ Used learned context from memory

âœ… STEP 3: Validating plan with MemAgent...
   âœ… Plan is VALID
   âœ“ Preconditions checked
   âœ“ Conflicts checked

================================================================================
ğŸ”” ITERATION 1: APPROVAL REQUIRED
================================================================================

ğŸ“‹ PROPOSED PLAN:
[STATE s0]
Current: MCP running, orchestrator not exists
...
[ACTION] Create directory structure
...

âœ… MEMAGENT VALIDATION:
âœ… VALID - All checks passed

ğŸ’¡ OPTIONS:
  y     - Approve and execute plan
  n     - Reject plan (will learn from this)
  edit  - Provide corrective feedback

ğŸ‘‰ Your decision (y/n/edit): y

âš™ï¸  STEP 5: Executing plan...
   â³ Executing actions...
   âœ“ Action 1 completed
   âœ“ Action 2 completed

ğŸ’¾ STEP 6: Writing success to memory (learning!)...
   âœ… Execution log updated
   âœ… Successful patterns recorded
   âœ… Memory enriched with learned context

âœ… Plan approved and executed successfully!
   Memory updated with learned context.

ğŸ” Continue to next planning cycle? (y/n): y

================================================================================
ğŸ”„ ITERATION 2/15
================================================================================

ğŸ“š STEP 1: Retrieving context from memory...
   âœ“ Successful patterns: 420 chars (learned from iteration 1!)
   ...
```

## Memory Files

The orchestrator creates/updates these memory entities:

### `entities/execution_log.md`
Tracks all successful iterations. Next iterations retrieve this as learned context.

```markdown
## Iteration 1 - SUCCESS âœ…
**Goal:** Implement orchestrator
**Plan:** Create directory â†’ Implement code
**Outcome:** Successfully executed
```

### `entities/successful_patterns.md`
Proven approaches that work well.

```markdown
### Pattern 1
**Approach:** Infrastructure setup (directory â†’ code)
**Result:** SUCCESS âœ…
**Learning:** This approach works, follow it
```

### `entities/planning_errors.md`
Mistakes to avoid (from rejected plans).

```markdown
### Error 1 - REJECTED âŒ
**Plan:** Deploy without testing
**Issue:** Violates KPMG procedures
**Lesson:** Always include testing phase
```

## Key Differences from Paper

| **Paper (Fine-Tuning)** | **Our System (Inference)** |
|-------------------------|----------------------------|
| Updates model weights | Updates memory context |
| Requires 40GB+ VRAM | Requires 1-2GB temporary |
| 30 hours training | Runs immediately |
| Specialized model | General model + memory |
| Learning in weights | Learning in context |

## Learning Process

### Iteration 1 (Cold Start)
- **Context**: Minimal (just project status)
- **Planning**: Basic approach
- **Outcome**: May need corrections

### Iteration 5 (Warming Up)
- **Context**: 4 successful patterns + error patterns
- **Planning**: Uses proven approaches
- **Outcome**: Higher success rate

### Iteration 15 (Expert)
- **Context**: 10+ successful patterns + known errors
- **Planning**: Sophisticated, avoids pitfalls
- **Outcome**: Consistently good plans

## Customization

### Change Max Iterations

```python
orchestrator = LearningOrchestrator(
    memory_path=memory_path,
    max_iterations=20  # Default: 15 (Î· in paper)
)
```

### Custom Goal

```python
orchestrator.run_learning_loop(
    goal="Your custom planning goal here"
)
```

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Orchestrator Loop (Python script)  â”‚
â”‚  Resource: ~500MB RAM               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Llama 3.3 70B (Already running)   â”‚
â”‚  Mac: Fireworks API                 â”‚
â”‚  H100: vLLM (80GB)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MemAgent Memory (Markdown files)   â”‚
â”‚  - execution_log.md                 â”‚
â”‚  - successful_patterns.md           â”‚
â”‚  - planning_errors.md               â”‚
â”‚  THIS IS WHERE LEARNING HAPPENS!    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Troubleshooting

### "Agent not found" error
Make sure you're in the `mem-agent-mcp/orchestrator/` directory and the parent directory has the `agent` module.

### "Memory path not found" error
Create a `.memory_path` file in `mem-agent-mcp/` with your memory directory path.

### High memory usage on H100
Each inference call uses 1-2GB temporarily. If you're hitting limits, reduce batch size or wait between calls.

## Next Steps

1. Test locally on Mac with Fireworks
2. Verify memory accumulation works
3. Run 5-10 iterations to build learned context
4. Transfer to H100 instance
5. Run production planning sessions

## Paper Reference

Based on: "Teaching LLMs to Plan: Logical Chain-of-Thought Instruction Tuning for Symbolic Planning" (Verma et al., MIT CSAIL)

Key adaptations:
- Inference-only (no fine-tuning)
- Memory-based learning (no weight updates)
- Human-in-the-loop approval
- Domain-specific (Project Jupiter)

